{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8031498,"sourceType":"datasetVersion","datasetId":4733994}],"dockerImageVersionId":30702,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\n!pip install pip==24.0\n!pip install accelerate==0.28.0\n!pip install bitsandbytes==0.43.0\n!pip install numpy==1.26.4\n!pip install pandas==2.2.1\n!pip install scikit-learn==1.4.1.post1\n!pip install scikit-multilearn==0.2.0\n!pip install transformers==4.38.2\n!pip install peft==0.9.0\n!pip install torch==2.2.1\n\"\"\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-23T22:09:56.241910Z","iopub.execute_input":"2024-04-23T22:09:56.242443Z","iopub.status.idle":"2024-04-23T22:09:56.253207Z","shell.execute_reply.started":"2024-04-23T22:09:56.242404Z","shell.execute_reply":"2024-04-23T22:09:56.251776Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'\\n!pip install pip==24.0\\n!pip install accelerate==0.28.0\\n!pip install bitsandbytes==0.43.0\\n!pip install numpy==1.26.4\\n!pip install pandas==2.2.1\\n!pip install scikit-learn==1.4.1.post1\\n!pip install scikit-multilearn==0.2.0\\n!pip install transformers==4.38.2\\n!pip install peft==0.9.0\\n!pip install torch==2.2.1\\n'"},"metadata":{}}]},{"cell_type":"code","source":"!python -m pip install --upgrade pip\n!pip install peft\n!pip install bitsandbytes\n!pip install accelerate\n!pip install --upgrade -q wandb","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\n\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\n# I have saved my API token with \"wandb_api\" as Label. \n# If you use some other Label make sure to change the same below. \nwandb_api = user_secrets.get_secret(\"wandb_api\") \n\nwandb.login(key=wandb_api)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport functools\nimport csv\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nfrom skmultilearn.model_selection import iterative_train_test_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\nfrom datasets import Dataset, DatasetDict\nfrom peft import (\n    LoraConfig,\n    prepare_model_for_kbit_training,\n    get_peft_model\n)\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n    get_linear_schedule_with_warmup\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set random seed for reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load dataset\ndf = pd.read_csv('/kaggle/input/baseline-dataset/baseline_dataset.csv')\n\n#df grab 3000 rows from each category, with 1500 of each label\ndf = df.groupby('category').head(10000).groupby('label').head(5000)\n\n# print the distrubution of the dataset\nprint(df['category'].value_counts())\nprint(df['label'].value_counts())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop('category', axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dropna(inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df display unique values in label column\nprint(df['label'].unique())\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split into text and labels\ntext = df['claim'].values\nlabels = df['label'].values\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Class weights for binary classification\nclass_counts = np.bincount(labels)\nclass_weights = torch.tensor([len(labels) / class_counts[1], len(labels) / class_counts[0]], dtype=torch.float32)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train-test split\nx_train, x_val, y_train, y_val = train_test_split(text, labels, test_size=0.1, stratify=labels, random_state=42)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Hugging Face datasets\nds = DatasetDict({\n    'train': Dataset.from_dict({'text': x_train, 'labels': y_train}),\n    'val': Dataset.from_dict({'text': x_val, 'labels': y_val})\n})\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model name\nmodel_name = 'mistralai/Mistral-7B-v0.1'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess dataset with tokenizer\ndef tokenize_examples(examples, tokenizer):\n    tokenized_inputs = tokenizer(examples['text'], truncation=True, padding=True, max_length=512)\n    tokenized_inputs['labels'] = examples['labels']\n    return tokenized_inputs\n\ntokenized_ds = ds.map(functools.partial(tokenize_examples, tokenizer=tokenizer), batched=True)\ntokenized_ds = tokenized_ds.with_format('torch')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# qunatization config\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit = True, # enable 4-bit quantization\n    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n    bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n)\n\n# lora config\nlora_config = LoraConfig(\n    r = 16, # the dimension of the low-rank matrices\n    lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n    lora_dropout = 0.05, # dropout probability of the LoRA layers\n    bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n    task_type = 'SEQ_CLS'\n)\n\n# load model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    quantization_config=quantization_config,\n    num_labels=1\n)\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\nmodel.config.pad_token_id = tokenizer.pad_token_id\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define custom batch preprocessor\ndef collate_fn(batch, tokenizer):\n    dict_keys = ['input_ids', 'attention_mask', 'labels']\n    d = {k: [dic[k] for dic in batch] for k in dict_keys}\n    d['input_ids'] = torch.nn.utils.rnn.pad_sequence(\n        d['input_ids'], batch_first=True, padding_value=tokenizer.pad_token_id\n    )\n    d['attention_mask'] = torch.nn.utils.rnn.pad_sequence(\n        d['attention_mask'], batch_first=True, padding_value=0\n    )\n    d['labels'] = torch.stack(d['labels'])\n    return d\n\n# Custom Trainer for handling class weights in binary classification\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        if logits.shape[-1] == 1:\n            logits = logits.squeeze(-1)\n        labels = labels.float()\n        loss_fn = torch.nn.BCEWithLogitsLoss()\n        loss = loss_fn(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n# Metrics computation for binary classification\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = torch.sigmoid(torch.tensor(predictions)).numpy()\n    predictions = np.round(predictions)  # Threshold at 0.5\n    precision = precision_score(labels, predictions)\n    recall = recall_score(labels, predictions)\n    f1 = f1_score(labels, predictions, average='weighted')\n    accuracy = accuracy_score(labels, predictions)\n\n    metrics = {\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1,\n        'accuracy': accuracy\n    }\n\n    wandb.log(metrics)\n    return metrics\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define training args with gradient accumulation, learning rate scheduler, and early stopping\ntraining_args = TrainingArguments(\n    output_dir='baseline_binary_mistral_10000',\n    learning_rate=2e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    gradient_accumulation_steps=2,\n    num_train_epochs=4,\n    weight_decay=0.01,\n    evaluation_strategy='steps',\n    eval_steps=50,\n    save_steps=50,\n    load_best_model_at_end=True,\n    logging_steps=100,\n    fp16=True,\n    report_to='wandb',  # Enable wandb logging\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the number of training steps for the scheduler\nnum_training_steps = (len(tokenized_ds['train']) // training_args.per_device_train_batch_size) * training_args.num_train_epochs\n\n# Initialize optimizer and scheduler\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=0, \n    num_training_steps=num_training_steps\n)\n\n# Initialize the Trainer\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_ds['train'],\n    eval_dataset=tokenized_ds['val'],\n    tokenizer=tokenizer,\n    data_collator=functools.partial(collate_fn, tokenizer=tokenizer),\n    compute_metrics=compute_metrics,\n    optimizers=(optimizer, scheduler)\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.object = object\n\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]}]}